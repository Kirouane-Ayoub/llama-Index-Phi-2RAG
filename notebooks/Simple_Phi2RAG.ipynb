{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U git+https://github.com/huggingface/transformers.git"
      ],
      "metadata": {
        "id": "6Vny8Z-eIqzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pypdf\n",
        "!pip install -q python-dotenv\n",
        "!pip install -q llama-index\n",
        "!pip install -q gradio\n",
        "!pip install einops\n",
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "3kiH95SuTwGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
        "from llama_index.llms import HuggingFaceLLM\n",
        "import torch\n",
        "\n",
        "documents = SimpleDirectoryReader(\"/content/Data\").load_data()"
      ],
      "metadata": {
        "id": "vs122vqGTsBW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.prompts.prompts import SimpleInputPrompt\n",
        "system_prompt = \"As a Q&A assistant, your primary objective is to respond to inquiries with the utmost precision, concentrating on delivering accurate answers in alignment with the given instructions and context.\"\n",
        "query_wrapper_prompt = SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")\n",
        "llm = HuggingFaceLLM(\n",
        "    context_window=4096,\n",
        "    max_new_tokens=100,\n",
        "    generate_kwargs={\"temperature\": 0.3, \"do_sample\": False},\n",
        "    system_prompt=system_prompt,\n",
        "    query_wrapper_prompt=query_wrapper_prompt,\n",
        "    tokenizer_name=\"microsoft/phi-2\",\n",
        "    model_name=\"microsoft/phi-2\",\n",
        "    device_map=\"cuda\",\n",
        "    model_kwargs={\"torch_dtype\": torch.bfloat16}\n",
        ")\n",
        "from llama_index.embeddings import HuggingFaceEmbedding\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "service_context = ServiceContext.from_defaults(\n",
        "    chunk_size=256,\n",
        "    llm=llm,\n",
        "    embed_model=embed_model\n",
        ")\n",
        "\n",
        "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "\n",
        "query_engine = index.as_query_engine()\n",
        "\n",
        "def predict(input, history):\n",
        "  response = query_engine.query(input)\n",
        "  return str(response)"
      ],
      "metadata": {
        "id": "ReohpVquTmeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gradio as gr\n",
        "\n",
        "gr.ChatInterface(predict).launch(share=True)"
      ],
      "metadata": {
        "id": "gZyvBS0STcQX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}